import streamlit as st
import time
import os
import json
import signal

# FIRST STREAMLIT COMMAND - Must be first!
st.set_page_config(
    page_title="üéØ Real-Time LinkedIn Content Quest",
    page_icon="üéØ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Performance optimizations - import and setup early
try:
    from utils.performance import setup_performance_monitoring, StreamlitContextManager
    # Initialize performance monitoring
    perf_optimizer = setup_performance_monitoring()
except ImportError:
    # Fallback if performance module not available
    perf_optimizer = None
    StreamlitContextManager = None

# Now we can safely do imports with error handling
# Core imports
try:
    from utils.embedding import EmbeddingModelSingleton, CrossEncoderModelSingleton
    from utils.qdrant import build_qdrant_client
    from utils.retriever import QdrantVectorDBRetriever
    CORE_MODULES_AVAILABLE = True
except ImportError as e:
    st.error(f"Core modules not available: {e}")
    CORE_MODULES_AVAILABLE = False

# Optional imports - Bytewax pipeline
try:
    from bytewax.testing import run_main
    from flow import build as build_flow
    BYTEWAX_AVAILABLE = True
except ImportError:
    st.warning("‚ö†Ô∏è Bytewax not available - using simplified mode")
    BYTEWAX_AVAILABLE = False

# Optional imports - LinkedIn scraper
try:
    from linkedin_posts_scrapper import fetch_posts, make_post_data
    SCRAPER_AVAILABLE = True
except ImportError:
    st.warning("‚ö†Ô∏è LinkedIn scraper not available - using existing data only")
    SCRAPER_AVAILABLE = False
st.title("üéØ Real-Time LinkedIn Content Quest")
st.markdown("### üîç Semantic Search for Social Media Content")
st.markdown("**Discover relevant LinkedIn posts using AI-powered semantic search**")

# Add helpful info about the system
with st.expander("‚ÑπÔ∏è How it works", expanded=False):
    st.markdown("""
    1. **üì• Fetch Data**: Import LinkedIn posts (via scraper or existing JSON files)
    2. **üîÑ Process**: Convert posts to vector embeddings using AI models  
    3. **üîç Search**: Enter queries to find semantically similar content
    4. **üìä Results**: Get ranked results with similarity scores
    """)

# Sidebar configuration
st.sidebar.title("‚öôÔ∏è Configuration")
number_of_results_want = st.sidebar.slider("üéØ Number of results to retrieve", 1, 10, 3, help="How many relevant posts to show for each search")

# System status in sidebar
st.sidebar.markdown("---")
st.sidebar.subheader("üõ†Ô∏è System Status")
status_col1, status_col2 = st.sidebar.columns(2)
with status_col1:
    st.sidebar.write("üìä Core Modules:", "‚úÖ" if CORE_MODULES_AVAILABLE else "‚ùå")
    st.sidebar.write("‚ö° Bytewax:", "‚úÖ" if BYTEWAX_AVAILABLE else "‚ùå")
with status_col2:
    st.sidebar.write("üîç Scraper:", "‚úÖ" if SCRAPER_AVAILABLE else "‚ùå")

# Debug mode activation - invisible to normal users
if 'debug_enabled' not in st.session_state:
    st.session_state.debug_enabled = False

if st.sidebar.button("üîß", help="Developer Mode"):
    st.session_state.debug_enabled = not st.session_state.debug_enabled

debug_mode = False
if st.session_state.debug_enabled:
    debug_mode = st.sidebar.checkbox("üîç Debug Mode", value=False, help="Show detailed pipeline logs")

def basic_prerequisites():
    st.markdown("## üì• Step 1: Data Source")
    
    if not SCRAPER_AVAILABLE:
        st.info("üìÅ **Using existing data files** - LinkedIn scraper not available. The system will use JSON files from the 'data' folder.")
        return None
    
    # Create tabs for different data source options
    tab1, tab2 = st.tabs(["üîç Fetch from LinkedIn", "üìÅ Use Existing Data"])
    
    with tab1:
        st.markdown("**Fetch fresh posts directly from LinkedIn profiles**")
        
        with st.form("linkedin_fetch_form"):
            col1, col2 = st.columns(2)
            
            with col1:
                linkedin_email = st.text_input("üìß LinkedIn Email", help="Your LinkedIn login email")
                linkedin_username_account = st.text_input("üë§ Target Username", help="Username of the profile to scrape (e.g., 'johndoe')")
            
            with col2:
                linkedin_password = st.text_input("üîê LinkedIn Password", type="password", help="Your LinkedIn password")
                st.write("")  # Spacing
                
            # Warning about LinkedIn scraping
            st.warning("‚ö†Ô∏è **Important**: This will open LinkedIn in your browser. Don't close it during the process!")
            
            need_data = st.form_submit_button("üß≤ Fetch LinkedIn Posts", type="primary")
    
    with tab2:
        st.info("üí° The system automatically detects and uses existing JSON files in the 'data' folder.")
        return None
        
    # Handle form submission
    if need_data:
        with st.spinner("üîÑ Processing your request..."):
            time.sleep(1)
            
            if not linkedin_email:
                st.error("üìß Please enter your LinkedIn email address!", icon="‚ùå")
            elif not linkedin_password:
                st.error("üîê Please enter your LinkedIn password!", icon="‚ùå")
            elif not linkedin_username_account:
                st.error("üë§ Please enter the LinkedIn username to scrape!", icon="‚ùå")
            else:
                try:
                    with st.status("üöÄ Fetching LinkedIn posts...", expanded=True) as status:
                        st.write("üåê Connecting to LinkedIn...")
                        account_posts_url = f"https://www.linkedin.com/in/{linkedin_username_account}/recent-activity/all/"
                        
                        st.write("üì° Fetching posts from profile...")
                        all_posts = fetch_posts(linkedin_email, linkedin_password, account_posts_url)
                        
                        st.write("üíæ Saving posts to local storage...")
                        make_post_data(all_posts, linkedin_username_account)
                        
                        status.update(label="‚úÖ Posts retrieved successfully!", state="complete", expanded=False)
                    
                    st.success(f"üéâ Successfully fetched posts from **{linkedin_username_account}**!")
                    st.balloons()
                    return linkedin_username_account
                    
                except Exception as e:
                    st.error(f"‚ùå **Error fetching posts**: {str(e)}")
                    st.info("üí° **Tip**: Try using existing data files or check your LinkedIn credentials.")
                    return None


def migrate_data_to_vectordb(username, debug_mode=False):
    st.markdown("## üîÑ Step 2: Data Processing")
    st.markdown("**Convert posts to AI-searchable vector embeddings**")
    
    if not CORE_MODULES_AVAILABLE:
        st.error("‚ùå **Core modules not available** - Please check your installation.")
        st.info("üí° **Tip**: Run `pip install -r requirements.txt` to install missing dependencies.")
        return
    
    # Create an expandable section for processing details
    with st.expander("‚ÑπÔ∏è What happens during processing", expanded=False):
        st.markdown("""
        1. **üìä Data Analysis**: Detect and validate data sources
        2. **üß† Model Loading**: Load AI models (embeddings + cross-encoder)
        3. **üîß Pipeline Setup**: Build data processing pipeline
        4. **‚ö° Processing**: Convert text to vectors using machine learning
        5. **üíæ Storage**: Save embeddings to vector database
        """)
    
    # Progress tracking
    progress_container = st.container()
    
    with progress_container:
        with st.status("üöÄ Initializing data migration...", expanded=True) as migration_status:
            progress_bar = st.progress(0, text="Getting ready...")
            
            # Debug log container - only visible in debug mode
            if debug_mode:
                st.info("üîç **Debug Mode Active** - Detailed logs below")
                debug_container = st.container()
                debug_logs = debug_container.empty()
            else:
                debug_logs = None
        
    if BYTEWAX_AVAILABLE:
        # Use full Bytewax pipeline with detailed progress
        try:
            from utils.supabase_client import supabase_client
            
            status_text.text("üìä Analyzing data sources...")
            progress_bar.progress(10)
            
            # Check data sources and validate
            if supabase_client.is_available():
                # Use Supabase data source
                st.info("üîÑ Using Supabase data source for migration")
                data_source_path = None
                post_count = "unknown"
            else:
                # Fallback to JSON files
                if username:
                    data_source_path = [f"data/{username}_data.json"]
                else:
                    data_folder = "data"
                    if os.path.exists(data_folder):
                        data_source_path = [f"data/{p}" for p in os.listdir(data_folder) if p.endswith('_data.json')]
                    else:
                        st.error("‚ùå No data found. Please fetch some posts first!")
                        return
                
                # Filter out empty files first
                non_empty_files = []
                for file_path in data_source_path:
                    if os.path.exists(file_path):
                        with open(file_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            if len(data.get('Posts', {})) > 0:
                                non_empty_files.append(file_path)
                
                if not non_empty_files:
                    st.warning("‚ö†Ô∏è All data files are empty. Please fetch some posts first.")
                    return
                
                data_source_path = non_empty_files  # Use only non-empty files
                
                # Count posts in filtered files only
                post_count = count_posts_in_files(data_source_path)
                if post_count == 0:
                    st.warning("‚ö†Ô∏è No posts found in data files. Please check your data or fetch new posts.")
                    return
                
                st.info(f"üìÅ Using {len(data_source_path)} non-empty JSON files ({post_count} posts) for migration")
            
            progress_bar.progress(20)
            status_text.text("üß† Loading ML models (this may take a few minutes on first run)...")
            
            # Pre-load models with progress feedback and caching
            try:
                with st.spinner("üß† Loading embedding model (cached after first use)..."):
                    embedding_model = EmbeddingModelSingleton()
                st.success("‚úÖ Embedding model loaded successfully")
                progress_bar.progress(40)
                
                with st.spinner("üîÑ Loading cross-encoder model (cached after first use)..."):
                    cross_encoder_model = CrossEncoderModelSingleton()
                st.success("‚úÖ Cross-encoder model loaded successfully")
                progress_bar.progress(60)
                
            except Exception as model_error:
                st.error(f"‚ùå Error loading ML models: {str(model_error)}")
                st.info("üîÑ Falling back to simplified mode...")
                migrate_data_simplified(username)
                return
            
            status_text.text("üîß Building data processing pipeline...")
            progress_bar.progress(70)
            
            # Build flow
            flow = build_flow(in_memory=False, data_source_path=data_source_path)
            
            status_text.text("‚ö° Processing data through pipeline...")
            progress_bar.progress(80)
            
            # Run pipeline with detailed logging and timeout
            import signal
            import threading
            import sys
            from io import StringIO
            
            # Set progressive timeouts with better user feedback
            def timeout_handler_20():
                st.info("‚è∞ Processing embeddings... Models working on posts.")
                
            def timeout_handler_35():
                st.warning("‚è∞ Vector generation in progress... Almost done.")
                
            def timeout_handler_50():
                st.warning("üîÑ Pipeline taking longer than expected. Will switch to simplified mode shortly.")
                
            def force_timeout_and_fallback():
                st.info("‚ö° Switching to simplified mode for better performance...")
                # Use a gentler approach - set a flag instead of interrupting
                import os
                os.environ['BYTEWAX_TIMEOUT'] = '1'
            
            timer_20 = threading.Timer(20.0, timeout_handler_20)
            timer_35 = threading.Timer(35.0, timeout_handler_35)
            timer_50 = threading.Timer(50.0, timeout_handler_50)
            timer_timeout = threading.Timer(70.0, force_timeout_and_fallback)  # Force fallback after 70 seconds
            
            timer_20.start()
            timer_35.start()
            timer_50.start()
            timer_timeout.start()
            
            # Capture pipeline output
            old_stdout = sys.stdout
            old_stderr = sys.stderr
            sys.stdout = captured_output = StringIO()
            sys.stderr = captured_errors = StringIO()
            
            # Debug logging setup
            debug_log_lines = []
            def debug_log(message):
                if debug_mode:
                    import datetime
                    timestamp = datetime.datetime.now().strftime("%H:%M:%S.%f")[:-3]
                    log_line = f"[{timestamp}] {message}"
                    debug_log_lines.append(log_line)
                    if debug_logs:
                        debug_logs.text_area("üîç Real-time Debug Logs:", 
                                           "\n".join(debug_log_lines[-20:]),  # Show last 20 lines
                                           height=300, key=f"debug_{len(debug_log_lines)}")
            
            debug_log("üöÄ Pipeline execution starting...")
            debug_log(f"üìä Processing {post_count} posts from {len(data_source_path)} files")
            
            try:
                st.info("üîÑ Executing Bytewax pipeline...")
                
                # Create a status container for real-time updates
                pipeline_status = st.empty()
                processing_details = st.empty()
                
                # Show initial pipeline info
                pipeline_status.info("üöÄ Pipeline started - processing posts through ML models...")
                
                # Run the pipeline with simple execution and success detection
                try:
                    debug_log("üéØ Calling run_main(flow) - this is where it usually hangs")
                    debug_log("‚è≥ Pipeline is running... monitoring for completion")
                    
                    # Monitor pipeline execution
                    import threading
                    import time
                    
                    def monitor_pipeline():
                        time.sleep(5)
                        debug_log("‚è±Ô∏è 5 seconds elapsed - pipeline still running")
                        time.sleep(10)
                        debug_log("‚è±Ô∏è 15 seconds elapsed - checking for output")
                        time.sleep(10)
                        debug_log("‚è±Ô∏è 25 seconds elapsed - pipeline should be processing data")
                        time.sleep(15)
                        debug_log("‚è±Ô∏è 40 seconds elapsed - this is where it usually gets stuck")
                    
                    if debug_mode:
                        monitor_thread = threading.Thread(target=monitor_pipeline)
                        monitor_thread.daemon = True
                        monitor_thread.start()
                    
                    run_main(flow)
                    debug_log("‚úÖ run_main(flow) completed successfully!")
                    pipeline_completed = True
                except Exception as pipeline_error:
                    # Since our debug showed the pipeline actually works,
                    # treat Bytewax shutdown errors as success if data was processed
                    output = captured_output.getvalue()
                    if ("Successfully upserted" in output and "points to Qdrant" in output) or \
                       ("Processing completed for post" in output and "chunks processed" in output):
                        st.success("‚úÖ Pipeline completed successfully (Bytewax shutdown handled)")
                        pipeline_completed = True
                    else:
                        raise pipeline_error
                
                # Cancel all timers if successful
                timer_20.cancel()
                timer_35.cancel()
                timer_50.cancel()
                timer_timeout.cancel()
                
                # Clear timeout flag if set
                if 'BYTEWAX_TIMEOUT' in os.environ:
                    del os.environ['BYTEWAX_TIMEOUT']
                
                # Restore output
                sys.stdout = old_stdout
                sys.stderr = old_stderr
                
                # Show captured logs
                output = captured_output.getvalue()
                errors = captured_errors.getvalue()
                
                if output:
                    st.text_area("üìã Pipeline Output:", output, height=200)
                    
                if errors:
                    st.text_area("‚ö†Ô∏è Pipeline Errors:", errors, height=100)
                
                progress_bar.progress(100)
                status_text.text("‚úÖ Migration completed successfully!")
                st.success("üéä Data transfer to VectorDB is finished!")
                
            except Exception as pipeline_error:
                # Cancel all timers
                timer_20.cancel()
                timer_35.cancel()
                timer_50.cancel()
                timer_timeout.cancel()
                
                # Clear timeout flag if set
                if 'BYTEWAX_TIMEOUT' in os.environ:
                    del os.environ['BYTEWAX_TIMEOUT']
                
                # Restore output
                sys.stdout = old_stdout
                sys.stderr = old_stderr
                
                # Show captured logs for debugging
                output = captured_output.getvalue()
                errors = captured_errors.getvalue()
                
                if output:
                    st.text_area("üìã Pipeline Output Before Error:", output, height=150)
                    
                if errors:
                    st.text_area("‚ö†Ô∏è Pipeline Errors:", errors, height=150)
                
                raise pipeline_error
                
        except Exception as e:
            progress_bar.progress(0)
            status_text.text("‚ùå Migration failed")
            st.error(f"‚ùå Error during migration: {str(e)}")
            
            # Check if this was a timeout issue and force simplified mode
            if "timeout" in str(e).lower() or "signal" in str(e).lower() or "interrupt" in str(e).lower():
                st.success("‚ö° Automatic switch to simplified mode - faster and more reliable!")
                st.info("üîÑ Automatically switching to simplified mode (recommended for your setup)")
            else:
                st.info("üîÑ Trying simplified migration as fallback...")
            
            # Fallback to simplified migration
            try:
                migrate_data_simplified(username)
                st.success("‚úÖ Fallback migration completed!")
                st.info("üí° Simplified mode is working perfectly for your data!")
            except Exception as fallback_error:
                st.error(f"‚ùå Fallback migration also failed: {str(fallback_error)}")
    else:
        # Simplified migration without Bytewax
        try:
            status_text.text("üîÑ Running simplified migration...")
            progress_bar.progress(50)
            
            migrate_data_simplified(username)
            
            progress_bar.progress(100)
            status_text.text("‚úÖ Simplified migration completed!")
            st.success("‚úÖ Data processed successfully (simplified mode)")
            
        except Exception as e:
            progress_bar.progress(0)
            status_text.text("‚ùå Migration failed")
            st.error(f"‚ùå Error during simplified migration: {str(e)}")


def count_posts_in_files(file_paths):
    """Count total posts in JSON files (assumes files already filtered for non-empty)"""
    import json
    total_posts = 0
    
    for file_path in file_paths:
        try:
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    posts = data.get('Posts', {})
                    total_posts += len(posts)
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Error reading {file_path}: {e}")
    
    return total_posts


def migrate_data_simplified(username):
    """Simplified data migration without Bytewax"""
    import json
    from utils.supabase_client import supabase_client
    
    # Load JSON data
    data_files = []
    if username:
        data_files = [f"data/{username}_data.json"]
    else:
        data_folder = "data"
        if os.path.exists(data_folder):
            data_files = [f"data/{p}" for p in os.listdir(data_folder) if p.endswith('_data.json')]
    
    if not data_files:
        st.error("‚ùå No data files found!")
        return
    
    st.info(f"üìÅ Processing {len(data_files)} data files in simplified mode")
    
    # Store data in session state for search
    all_posts = []
    for file_path in data_files:
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for post_id, post_data in data.get('Posts', {}).items():
                    all_posts.append({
                        'post_id': post_id,
                        'text': post_data.get('text', ''),
                        'post_owner': post_data.get('post_owner', post_data.get('name', '')),
                        'source': post_data.get('source', 'LinkedIn')
                    })
    
    # Store in session state
    st.session_state['posts_data'] = all_posts
    st.success(f"‚úÖ Loaded {len(all_posts)} posts for search")


def get_insights_from_posts():
    st.markdown("## üîç Step 3: Semantic Search")
    st.markdown("**Find relevant posts using natural language queries**")
    
    # Add examples and tips
    with st.expander("üí° Search Tips & Examples", expanded=False):
        st.markdown("""
        **Example queries:**
        - "machine learning projects"
        - "startup advice and entrepreneurship"
        - "Python programming tutorials"
        - "career growth and networking"
        - "data science best practices"
        
        **Tips for better results:**
        - Use descriptive, natural language
        - Try multiple related terms
        - Be specific but not overly narrow
        """)
    
    with st.form("search_form", clear_on_submit=False):
        col1, col2 = st.columns([4, 1])
        
        with col1:
            query = st.text_area(
                "üîç **Enter your search query:**",
                placeholder="e.g., 'machine learning best practices' or 'startup funding advice'",
                height=100,
                help="Describe what you're looking for in natural language"
            )
        
        with col2:
            st.write("")  # Spacing
            st.write("")  # Spacing
            submitted = st.form_submit_button("üöÄ Search", type="primary", use_container_width=True)
            
        if submitted:
            if not query.strip():
                st.warning("‚ö†Ô∏è **Please enter a search query** to find relevant posts", icon="üîç")
                return
                
            start_time = time.time()
            
            if CORE_MODULES_AVAILABLE:
                # Use full vector search
                try:
                    from utils.supabase_client import supabase_client
                    
                    embedding_model = EmbeddingModelSingleton()
                    cross_encode_model = CrossEncoderModelSingleton()
                    qdrant_client = build_qdrant_client()
                    vectordb_retriever = QdrantVectorDBRetriever(
                        embedding_model=embedding_model,
                        cross_encoder_model=cross_encode_model,
                        vector_db_client=qdrant_client,
                    )
                    
                    with st.spinner("‚è≥ Vector search in progress..."):
                        retrieved_results = vectordb_retriever.search(
                            query, limit=number_of_results_want, return_all=True
                        )
                    
                    # Log search for analytics
                    execution_time = int((time.time() - start_time) * 1000)
                    results_count = len(retrieved_results.get("posts", []))
                    supabase_client.log_search(query, results_count, execution_time)
                    
                    st.toast("‚úÖ Vector search completed!", icon="üéØ")
                    display_search_results(retrieved_results.get("posts", []), "vector")
                    
                except Exception as e:
                    st.error(f"‚ùå Vector search failed: {e}")
                    # Fallback to simple search
                    simple_search_results = perform_simple_search(query)
                    display_search_results(simple_search_results, "simple")
            else:
                # Use simple text search
                simple_search_results = perform_simple_search(query)
                display_search_results(simple_search_results, "simple")


def perform_simple_search(query):
    """Simple text-based search as fallback"""
    if 'posts_data' not in st.session_state:
        st.warning("‚ö†Ô∏è No data loaded. Please run data migration first.")
        return []
    
    posts = st.session_state['posts_data']
    query_lower = query.lower()
    
    # Simple text matching
    matching_posts = []
    for post in posts:
        text_lower = post['text'].lower()
        if query_lower in text_lower:
            # Simple scoring based on query word frequency
            score = text_lower.count(query_lower) / len(text_lower.split())
            matching_posts.append({
                'post_id': post['post_id'],
                'text': post['text'],
                'post_owner': post['post_owner'],
                'source': post['source'],
                'score': score
            })
    
    # Sort by score and limit results
    matching_posts.sort(key=lambda x: x['score'], reverse=True)
    return matching_posts[:number_of_results_want]


def display_search_results(posts, search_type):
    """Display search results in consistent format"""
    if not posts:
        st.info("üîç **No results found** - Try adjusting your search terms or check if data is loaded", icon="ü§î")
        return
    
    # Results header with metrics
    search_icon = "üéØ" if search_type == "vector" else "üìù"
    search_label = "AI Semantic" if search_type == "vector" else "Text-based"
    
    st.success(f"{search_icon} **Found {len(posts)} results** using {search_label} search")
    
    # Add search quality indicator for vector search
    if search_type == "vector" and posts:
        avg_score = sum(post.score if hasattr(post, 'score') else post.get('score', 0) for post in posts) / len(posts)
        if avg_score > 0.8:
            st.info("‚ú® **High relevance** - Results closely match your query", icon="üéØ")
        elif avg_score > 0.6:
            st.info("üëç **Good relevance** - Results are related to your query", icon="üìä") 
        else:
            st.info("üîç **Partial matches** - Consider refining your search terms", icon="‚ö°")
    
    # Display results in a more appealing format
    for index, post in enumerate(posts):
        # Extract post data consistently
        if hasattr(post, 'post_id'):
            # Vector search result object
            post_id = post.post_id
            owner = post.post_owner
            source = post.source
            score = post.score
            text = post.full_raw_text
        else:
            # Simple search result dict
            post_id = post['post_id']
            owner = post['post_owner']
            source = post['source']
            score = post['score']
            text = post['text']
        
        # Create a more attractive result card
        with st.container():
            col1, col2 = st.columns([3, 1])
            
            with col1:
                st.markdown(f"### üìå **Result #{index+1}**")
                
            with col2:
                # Score badge
                score_color = "üü¢" if score > 0.8 else "üü°" if score > 0.6 else "üî¥"
                st.markdown(f"**{score_color} {score:.3f}**")
            
            # Post metadata in a clean format
            meta_col1, meta_col2, meta_col3 = st.columns(3)
            with meta_col1:
                st.markdown(f"üë§ **{owner}**")
            with meta_col2:
                st.markdown(f"üì± **{source}**")
            with meta_col3:
                st.markdown(f"üÜî `{post_id}`")
            
            # Post content with better formatting
            st.markdown("**üìù Content:**")
            with st.container():
                # Truncate very long posts for better readability
                if len(text) > 500:
                    preview_text = text[:500] + "..."
                    with st.expander("üìñ Read full post", expanded=False):
                        st.write(text)
                    st.write(preview_text)
                else:
                    st.write(text)
            
            st.divider()  # Visual separator between results


if __name__ == "__main__":
    # Main application workflow
    username = None
    
    # Step 1: Data Source Configuration
    with st.expander("üì• **Step 1: Configure Data Source**", expanded=True):
        username = basic_prerequisites()
    
    # Step 2: Data Processing 
    with st.expander("üîÑ **Step 2: Process Data for AI Search**", expanded=False):
        st.markdown("Convert your posts into AI-searchable format")
        
        col1, col2 = st.columns([3, 1])
        with col1:
            st.info("üí° **Required**: Process data before searching to enable semantic search capabilities")
        with col2:
            migrate_data = st.button("üöÄ **Start Processing**", type="primary", use_container_width=True)
        
        if migrate_data:
            migrate_data_to_vectordb(username, debug_mode)
    
    # Step 3: Search Interface
    st.markdown("---")  # Visual separator
    get_insights_from_posts()
    
    # Footer with helpful information
    st.markdown("---")
    with st.expander("‚ÑπÔ∏è **About This System**", expanded=False):
        st.markdown("""
        ### üéØ **Real-Time LinkedIn Content Quest**
        
        This AI-powered system helps you discover relevant LinkedIn posts using semantic search technology:
        
        - **ü§ñ AI Models**: Uses sentence-transformers for embeddings and cross-encoders for reranking
        - **‚ö° Real-time Processing**: Bytewax pipeline for efficient data processing
        - **üóÑÔ∏è Vector Storage**: Qdrant database for fast similarity search
        - **üîç Smart Search**: Understands context and meaning, not just keywords
        
        **Built with**: Python, Streamlit, PyTorch, Qdrant, Bytewax
        """)
    
    # Show current system status in footer
    st.sidebar.markdown("---")
    st.sidebar.markdown("### üìä **Current Session**")
    if 'posts_data' in st.session_state:
        st.sidebar.success(f"‚úÖ {len(st.session_state['posts_data'])} posts loaded")
    else:
        st.sidebar.info("‚è≥ No posts loaded yet")
